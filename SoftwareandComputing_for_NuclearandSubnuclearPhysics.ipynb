{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoftwareandComputing_for_NuclearandSubnuclearPhysics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **MAGIC TELESCOPE**"
      ],
      "metadata": {
        "id": "ihRgisS6L7d1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Magic telescope is a Cherenkov gamma telescope placed in Roque de los Muchachos Observatory, at La Palma, which observes high energy gamma rays, taking advantage of the radiation emitted by charged particles, when they traversing the atmosphere. When electromagnetic showers are generated, gamma rays coming from the Cherenkov effect are produced. This Cherenkov radiation (from visible to UV wavelengths) leaks through the atmosphere and gets recorded in the Magic Telescope detector, allowing reconstruction of the shower parameters. The available information consists of pulses left by the incoming Cherenkov photons on the photomultiplier tubes, arranged in a plane, the camera. Depending on the energy of the primary gamma, in the range from few hundreds to some 10000 Cherenkov photons get collected in patterns (called the shower image) that allows to discriminate statistically those caused by primary gammas (signal) from the images of hadronic showers initiated by cosmic rays in the upper atmosphere (background). Usually, the image of a shower after some pre-processing is an elongated cluster. Its long axis is oriented towards the camera center if the shower axis is parallel to the telescope's optical axis, i.e. if the telescope axis is directed towards a point source. A principal component analysis is performed in the camera plane, which results in a correlation axis and defines an ellipse. If the depositions were distributed as a bivariate Gaussian, this would be an equidensity ellipse. The characteristic parameters of this ellipse (often called Hillas parameters) are among the image parameters that can be used for discrimination. The energy depositions are typically asymmetric along the major axis, and this asymmetry can also be used in discrimination. There are, in addition, further discriminating characteristics, like the extent of the cluster in the image plane, or the total sum of depositions. Through this methods it is possible to know if the shower is generated by a gamma or an hadron. This project aims predict the source of the shower through machine learning"
      ],
      "metadata": {
        "id": "BbASsi2mMEeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "%matplotlib inline\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "6jIWmkxVMJgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset **MagicTelescope**"
      ],
      "metadata": {
        "id": "hiCdcz7vMTmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data set used can be found on OpenML and was generated by a Mont Carlo generetor program called CORSIKA, that consider the imaging technique previously mentioned. The code simulate the registration of high energy gamma particles in a ground-based atmospheric Cherenkov gamma telescope and it was run with parameters allowing to observe events with energies below 50 GeV.\n",
        "\n",
        "Attribute Information:\n",
        "\n",
        "\n",
        "\n",
        "1.   fLength: continuous # major axis of ellipse [mm]\n",
        "2.   fWidth: continuous # minor axis of ellipse [mm]\n",
        "3.   fSize: continuous # 10-log of sum of content of all pixels [in #phot]\n",
        "4. fConc: continuous # ratio of sum of two highest pixels over fSize [ratio]\n",
        "5. fConc1: continuous # ratio of highest pixel over fSize [ratio]\n",
        "6. fAsym: continuous # distance from highest pixel to center, projected onto major axis [mm]\n",
        "7. fM3Long: continuous # 3rd root of third moment along major axis [mm]\n",
        "8. fM3Trans: continuous # 3rd root of third moment along minor axis [mm]\n",
        "9. fAlpha: continuous # angle of major axis with vector to origin [deg]\n",
        "10. fDist: continuous # distance from origin to center of ellipse [mm]\n",
        "11. class: g,h # gamma (signal), hadron (background)\n",
        "\n",
        "\n",
        "\n",
        "D. Heck et al., CORSIKA, A Monte Carlo code to simulate extensive air showers, Forschungszentrum Karlsruhe FZKA 6019 (1998)."
      ],
      "metadata": {
        "id": "qjj5dVpyNCrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = fetch_openml('MagicTelescope')"
      ],
      "metadata": {
        "id": "RUk0gFbiOE-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = data['data'], data['target']"
      ],
      "metadata": {
        "id": "XP9cbQBT2Omw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here there are some information about the dataset: its features name and the dataset shape."
      ],
      "metadata": {
        "id": "SHr8gkeZOUsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['feature_names'])\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CyIPNV22OXuz",
        "outputId": "bf5a71f1-1a54-4375-d30d-1a82a9f6318e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['fLength:', 'fWidth:', 'fSize:', 'fConc:', 'fConc1:', 'fAsym:', 'fM3Long:', 'fM3Trans:', 'fAlpha:', 'fDist:']\n",
            "(19020, 10) (19020,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To visualize first five rows of the dataset"
      ],
      "metadata": {
        "id": "BN8Udbq9l99T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(X[:5])\n",
        "print(y[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QJ-zWKNl4Yi",
        "outputId": "0d92def1-546e-4547-d1b7-ee47ef70c57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   fLength:   fWidth:  fSize:  fConc:  ...  fM3Long:  fM3Trans:  fAlpha:    fDist:\n",
            "0   28.7967   16.0021  2.6449  0.3918  ...   22.0110    -8.2027  40.0920   81.8828\n",
            "1   31.6036   11.7235  2.5185  0.5303  ...   23.8238    -9.9574   6.3609  205.2610\n",
            "2  162.0520  136.0310  4.0612  0.0374  ...  -64.8580   -45.2160  76.9600  256.7880\n",
            "3   23.8172    9.5728  2.3385  0.6147  ...   -6.4633    -7.1513  10.4490  116.7370\n",
            "4   75.1362   30.9205  3.1611  0.3168  ...   28.5525    21.8393   4.6480  356.4620\n",
            "\n",
            "[5 rows x 10 columns]\n",
            "0    g\n",
            "1    g\n",
            "2    g\n",
            "3    g\n",
            "4    g\n",
            "Name: class:, dtype: category\n",
            "Categories (2, object): ['g', 'h']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the count of events for backgrounf (h) and signal (g)"
      ],
      "metadata": {
        "id": "S8O8RiltBA7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y.value_counts(ascending=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bct3YbIuESIR",
        "outputId": "12006f66-5ead-476b-e76f-13e17ae6af4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "h     6688\n",
              "g    12332\n",
              "Name: class:, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,8))\n",
        "y.value_counts().plot(kind='bar', facecolor='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "tdV0dcN6Ifik",
        "outputId": "c13635f4-8b45-4121-ad32-5d0b1a0e9577"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f1392f34fd0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the y target consist of an array of strings ['h', 'g'], there is the necessity to to make this a \"One Hot\" encoding for the training. "
      ],
      "metadata": {
        "id": "Elqz8fuHlQVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "y = to_categorical(y,2)"
      ],
      "metadata": {
        "id": "wYBzfPLUlUpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the **model**"
      ],
      "metadata": {
        "id": "orLW5EnrSyZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model thai I chose is made by 3 hidden layers with 64, then 32, then 32 neurons. Each layer will use relu (Rectified Linear Unit) activation. ReLu is a linear function that will output the input directly if it is positive, otherwise, it will output zero. It is used in many neural networks because a model which uses it, is easier to train and often achieves better performance. It is add an output layer with 2 neurons (one for each class), then finish with Softmax activation."
      ],
      "metadata": {
        "id": "81gRPIoFSYk_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "Y-5oBkCzSl6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_shape=(10,), name='fc1', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "model.add(Activation(activation='relu', name='relu1'))\n",
        "model.add(Dense(32, name='fc2', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "model.add(Activation(activation='relu', name='relu2'))\n",
        "model.add(Dense(32, name='fc3', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "model.add(Activation(activation='relu', name='relu3'))\n",
        "model.add(Dense(2, name='output', kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
        "model.add(Activation(activation='softmax', name='softmax'))"
      ],
      "metadata": {
        "id": "CMlsTnIJWFoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training the model**"
      ],
      "metadata": {
        "id": "XeWdV0HYmbZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, the the dataset is split into training and validation sets:"
      ],
      "metadata": {
        "id": "kDTwU1dFP4Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_val = scaler.fit_transform(X_train_val)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "4pP6hDbhP3nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "pmpLIztvP270"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('X_train_val.npy', X_train_val)\n",
        "np.save('X_test.npy', X_test)\n",
        "np.save('y_train_val.npy', y_train_val)\n",
        "np.save('y_test.npy', y_test)\n",
        "np.save('classes.npy', le.classes_)"
      ],
      "metadata": {
        "id": "S8G0bPkYQEUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model I used the Adaptive Momentum estimation (Adam) optimizer with categorical crossentropy loss.   \n",
        "Adam is a method for efficient stochastic optimization that only requires first-order gradients with little memory requirement."
      ],
      "metadata": {
        "id": "BnzB0UZiWN6S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from callbacks import all_callbacks\n",
        "train = True\n",
        "if train:\n",
        "  adam = Adam(learning_rate=0.0001)\n",
        "  model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
        "  callbacks = all_callbacks(stop_patience = 1000,\n",
        "                              lr_factor = 0.5,\n",
        "                              lr_patience = 10,\n",
        "                              lr_epsilon = 0.000001,\n",
        "                              lr_cooldown = 2,\n",
        "                              lr_minimum = 0.0000001,\n",
        "                              outputDir = 'model_1')\n",
        "  model.fit(X_train_val, y_train_val, batch_size=1024, epochs=100, validation_split=0.25, shuffle=True,callbacks = callbacks.callbacks)\n",
        "else:\n",
        "  from tensorflow.keras.models import load_model\n",
        "  model = load_model('model_1/KERAS_check_best_model.h5')"
      ],
      "metadata": {
        "id": "iCnOalt6WToW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38b7fa28-835f-43e6-e772-c37ae9a67031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`epsilon` argument is deprecated and will be removed, use `min_delta` instead.\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
            "Epoch 1/100\n",
            " 1/12 [=>............................] - ETA: 10s - loss: 12.6019 - accuracy: 0.3760WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0040s vs `on_train_batch_end` time: 0.0271s). Check your callbacks.\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 10.2984 - accuracy: 0.3678 \n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 6.27498, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 6.27498, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00001: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00001: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 1s 36ms/step - loss: 10.2613 - accuracy: 0.3683 - val_loss: 6.2750 - val_accuracy: 0.4414 - lr: 1.0000e-04\n",
            "Epoch 2/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 6.5580 - accuracy: 0.4336\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00002: val_loss improved from 6.27498 to 3.13572, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00002: val_loss improved from 6.27498 to 3.13572, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00002: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00002: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 4.7466 - accuracy: 0.5053 - val_loss: 3.1357 - val_accuracy: 0.5778 - lr: 1.0000e-04\n",
            "Epoch 3/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 3.1221 - accuracy: 0.5869\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.13572 to 2.37943, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.13572 to 2.37943, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00003: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00003: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 2.7578 - accuracy: 0.6028 - val_loss: 2.3794 - val_accuracy: 0.6396 - lr: 1.0000e-04\n",
            "Epoch 4/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 2.4123 - accuracy: 0.6211\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.37943 to 1.99843, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.37943 to 1.99843, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00004: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00004: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 2.2215 - accuracy: 0.6544 - val_loss: 1.9984 - val_accuracy: 0.6711 - lr: 1.0000e-04\n",
            "Epoch 5/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.8797 - accuracy: 0.6953\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.99843 to 1.74575, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.99843 to 1.74575, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00005: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00005: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.8620 - accuracy: 0.6851 - val_loss: 1.7458 - val_accuracy: 0.6872 - lr: 1.0000e-04\n",
            "Epoch 6/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.7770 - accuracy: 0.6943\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.74575 to 1.64062, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.74575 to 1.64062, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00006: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00006: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 1.6687 - accuracy: 0.6979 - val_loss: 1.6406 - val_accuracy: 0.6990 - lr: 1.0000e-04\n",
            "Epoch 7/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.7298 - accuracy: 0.6943\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.64062 to 1.53439, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.64062 to 1.53439, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00007: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00007: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 1.5580 - accuracy: 0.7087 - val_loss: 1.5344 - val_accuracy: 0.7085 - lr: 1.0000e-04\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.4557 - accuracy: 0.7145\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.53439 to 1.43371, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.53439 to 1.43371, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00008: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00008: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.4557 - accuracy: 0.7145 - val_loss: 1.4337 - val_accuracy: 0.7140 - lr: 1.0000e-04\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 1.3588 - accuracy: 0.7155\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.43371 to 1.34153, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.43371 to 1.34153, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00009: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00009: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.3588 - accuracy: 0.7155 - val_loss: 1.3415 - val_accuracy: 0.7156 - lr: 1.0000e-04\n",
            "Epoch 10/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.3158 - accuracy: 0.7080\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.34153 to 1.25528, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.34153 to 1.25528, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00010: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00010: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00010: saving model to model_1/KERAS_check_model_epoch10.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 1.2684 - accuracy: 0.7140 - val_loss: 1.2553 - val_accuracy: 0.7127 - lr: 1.0000e-04\n",
            "Epoch 11/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.3142 - accuracy: 0.7090\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.25528 to 1.17652, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.25528 to 1.17652, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00011: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00011: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.1825 - accuracy: 0.7150 - val_loss: 1.1765 - val_accuracy: 0.7119 - lr: 1.0000e-04\n",
            "Epoch 12/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0963 - accuracy: 0.7305\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.17652 to 1.11111, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.17652 to 1.11111, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00012: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00012: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.1069 - accuracy: 0.7153 - val_loss: 1.1111 - val_accuracy: 0.7093 - lr: 1.0000e-04\n",
            "Epoch 13/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 1.0655 - accuracy: 0.7168\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.11111 to 1.05583, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00013: val_loss improved from 1.11111 to 1.05583, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00013: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00013: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.0438 - accuracy: 0.7127 - val_loss: 1.0558 - val_accuracy: 0.7077 - lr: 1.0000e-04\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.9924 - accuracy: 0.7148\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.05583 to 1.00875, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.05583 to 1.00875, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00014: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00014: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9924 - accuracy: 0.7148 - val_loss: 1.0087 - val_accuracy: 0.7079 - lr: 1.0000e-04\n",
            "Epoch 15/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9281 - accuracy: 0.7324\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.00875 to 0.96967, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00015: val_loss improved from 1.00875 to 0.96967, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00015: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00015: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.9496 - accuracy: 0.7176 - val_loss: 0.9697 - val_accuracy: 0.7077 - lr: 1.0000e-04\n",
            "Epoch 16/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8955 - accuracy: 0.7236\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.96967 to 0.93488, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.96967 to 0.93488, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00016: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00016: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.9141 - accuracy: 0.7170 - val_loss: 0.9349 - val_accuracy: 0.7090 - lr: 1.0000e-04\n",
            "Epoch 17/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.8852 - accuracy: 0.7194\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.93488 to 0.90599, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.93488 to 0.90599, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00017: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00017: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.8839 - accuracy: 0.7194 - val_loss: 0.9060 - val_accuracy: 0.7103 - lr: 1.0000e-04\n",
            "Epoch 18/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.9008 - accuracy: 0.7119\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.90599 to 0.88115, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.90599 to 0.88115, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00018: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00018: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.8572 - accuracy: 0.7179 - val_loss: 0.8812 - val_accuracy: 0.7103 - lr: 1.0000e-04\n",
            "Epoch 19/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8635 - accuracy: 0.7207\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.88115 to 0.85817, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.88115 to 0.85817, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00019: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00019: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.8348 - accuracy: 0.7207 - val_loss: 0.8582 - val_accuracy: 0.7106 - lr: 1.0000e-04\n",
            "Epoch 20/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7931 - accuracy: 0.7500\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.85817 to 0.83971, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.85817 to 0.83971, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00020: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00020: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00020: saving model to model_1/KERAS_check_model_epoch20.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8138 - accuracy: 0.7214 - val_loss: 0.8397 - val_accuracy: 0.7127 - lr: 1.0000e-04\n",
            "Epoch 21/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8429 - accuracy: 0.7021\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.83971 to 0.82172, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.83971 to 0.82172, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00021: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00021: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.7955 - accuracy: 0.7227 - val_loss: 0.8217 - val_accuracy: 0.7164 - lr: 1.0000e-04\n",
            "Epoch 22/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7413 - accuracy: 0.7197\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.82172 to 0.80404, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.82172 to 0.80404, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00022: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00022: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.7794 - accuracy: 0.7253 - val_loss: 0.8040 - val_accuracy: 0.7171 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.8008 - accuracy: 0.7119\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.80404 to 0.78793, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.80404 to 0.78793, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00023: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00023: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7631 - accuracy: 0.7245 - val_loss: 0.7879 - val_accuracy: 0.7174 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7674 - accuracy: 0.7148\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.78793 to 0.77120, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.78793 to 0.77120, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00024: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00024: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7483 - accuracy: 0.7259 - val_loss: 0.7712 - val_accuracy: 0.7190 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.7358 - accuracy: 0.7279\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.77120 to 0.75566, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.77120 to 0.75566, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00025: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00025: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.7346 - accuracy: 0.7279 - val_loss: 0.7557 - val_accuracy: 0.7242 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7142 - accuracy: 0.7188\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.75566 to 0.74238, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.75566 to 0.74238, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00026: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00026: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7214 - accuracy: 0.7307 - val_loss: 0.7424 - val_accuracy: 0.7258 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6842 - accuracy: 0.7432\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.74238 to 0.72983, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.74238 to 0.72983, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00027: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00027: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7089 - accuracy: 0.7271 - val_loss: 0.7298 - val_accuracy: 0.7227 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.7427 - accuracy: 0.7256\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.72983 to 0.71824, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.72983 to 0.71824, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00028: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00028: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6977 - accuracy: 0.7295 - val_loss: 0.7182 - val_accuracy: 0.7224 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6826 - accuracy: 0.7412\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.71824 to 0.70638, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.71824 to 0.70638, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00029: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00029: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6871 - accuracy: 0.7285 - val_loss: 0.7064 - val_accuracy: 0.7292 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6780 - accuracy: 0.7341\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.70638 to 0.69769, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.70638 to 0.69769, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00030: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00030: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00030: saving model to model_1/KERAS_check_model_epoch30.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6780 - accuracy: 0.7341 - val_loss: 0.6977 - val_accuracy: 0.7290 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6793 - accuracy: 0.7256\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.69769 to 0.68657, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.69769 to 0.68657, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00031: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00031: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6685 - accuracy: 0.7341 - val_loss: 0.6866 - val_accuracy: 0.7329 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6490 - accuracy: 0.7461\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.68657 to 0.67859, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.68657 to 0.67859, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00032: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00032: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6598 - accuracy: 0.7347 - val_loss: 0.6786 - val_accuracy: 0.7311 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6374 - accuracy: 0.7529\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.67859 to 0.67101, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.67859 to 0.67101, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00033: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00033: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6517 - accuracy: 0.7355 - val_loss: 0.6710 - val_accuracy: 0.7337 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6657 - accuracy: 0.7158\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.67101 to 0.66322, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.67101 to 0.66322, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00034: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00034: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6448 - accuracy: 0.7369 - val_loss: 0.6632 - val_accuracy: 0.7348 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6702 - accuracy: 0.7158\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.66322 to 0.65749, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.66322 to 0.65749, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00035: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00035: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6386 - accuracy: 0.7386 - val_loss: 0.6575 - val_accuracy: 0.7324 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6309 - accuracy: 0.7360\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.65749 to 0.65012, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.65749 to 0.65012, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00036: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00036: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.6326 - accuracy: 0.7357 - val_loss: 0.6501 - val_accuracy: 0.7434 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6227 - accuracy: 0.7402\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.65012 to 0.64601, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.65012 to 0.64601, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00037: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00037: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6266 - accuracy: 0.7396 - val_loss: 0.6460 - val_accuracy: 0.7382 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6516 - accuracy: 0.7275\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.64601 to 0.63994, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.64601 to 0.63994, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00038: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00038: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6212 - accuracy: 0.7399 - val_loss: 0.6399 - val_accuracy: 0.7429 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5959 - accuracy: 0.7412\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.63994 to 0.63593, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.63994 to 0.63593, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00039: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00039: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.6169 - accuracy: 0.7421 - val_loss: 0.6359 - val_accuracy: 0.7426 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.6121 - accuracy: 0.7429\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.63593 to 0.63201, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.63593 to 0.63201, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00040: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00040: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00040: saving model to model_1/KERAS_check_model_epoch40.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.6125 - accuracy: 0.7426 - val_loss: 0.6320 - val_accuracy: 0.7432 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6537 - accuracy: 0.7383\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.63201 to 0.62776, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.63201 to 0.62776, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00041: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00041: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6094 - accuracy: 0.7418 - val_loss: 0.6278 - val_accuracy: 0.7450 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6535 - accuracy: 0.7178\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.62776 to 0.62500, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.62776 to 0.62500, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00042: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00042: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.6058 - accuracy: 0.7441 - val_loss: 0.6250 - val_accuracy: 0.7440 - lr: 1.0000e-04\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.6019 - accuracy: 0.7461\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.62500 to 0.62226, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.62500 to 0.62226, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00043: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00043: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.6019 - accuracy: 0.7461 - val_loss: 0.6223 - val_accuracy: 0.7445 - lr: 1.0000e-04\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5992 - accuracy: 0.7461\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.62226 to 0.61900, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.62226 to 0.61900, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00044: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00044: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.5992 - accuracy: 0.7461 - val_loss: 0.6190 - val_accuracy: 0.7463 - lr: 1.0000e-04\n",
            "Epoch 45/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5969 - accuracy: 0.7472\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.61900 to 0.61488, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00045: val_loss improved from 0.61900 to 0.61488, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00045: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00045: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5954 - accuracy: 0.7475 - val_loss: 0.6149 - val_accuracy: 0.7445 - lr: 1.0000e-04\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5929 - accuracy: 0.7490\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.61488 to 0.61227, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.61488 to 0.61227, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00046: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00046: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.5929 - accuracy: 0.7490 - val_loss: 0.6123 - val_accuracy: 0.7450 - lr: 1.0000e-04\n",
            "Epoch 47/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5277 - accuracy: 0.7832\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.61227 to 0.60988, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.61227 to 0.60988, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00047: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00047: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5903 - accuracy: 0.7511 - val_loss: 0.6099 - val_accuracy: 0.7450 - lr: 1.0000e-04\n",
            "Epoch 48/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5834 - accuracy: 0.7715\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.60988 to 0.60833, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00048: val_loss improved from 0.60988 to 0.60833, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00048: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00048: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5877 - accuracy: 0.7521 - val_loss: 0.6083 - val_accuracy: 0.7442 - lr: 1.0000e-04\n",
            "Epoch 49/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5850 - accuracy: 0.7504\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.60833 to 0.60553, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.60833 to 0.60553, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00049: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00049: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5854 - accuracy: 0.7501 - val_loss: 0.6055 - val_accuracy: 0.7429 - lr: 1.0000e-04\n",
            "Epoch 50/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.5830 - accuracy: 0.7524\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.60553 to 0.60502, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.60553 to 0.60502, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00050: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00050: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00050: saving model to model_1/KERAS_check_model_epoch50.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.5830 - accuracy: 0.7518 - val_loss: 0.6050 - val_accuracy: 0.7432 - lr: 1.0000e-04\n",
            "Epoch 51/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6090 - accuracy: 0.7490\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.60502 to 0.60141, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.60502 to 0.60141, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00051: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00051: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5806 - accuracy: 0.7530 - val_loss: 0.6014 - val_accuracy: 0.7440 - lr: 1.0000e-04\n",
            "Epoch 52/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5778 - accuracy: 0.7528\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.60141 to 0.59932, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00052: val_loss improved from 0.60141 to 0.59932, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00052: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00052: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.5778 - accuracy: 0.7525 - val_loss: 0.5993 - val_accuracy: 0.7426 - lr: 1.0000e-04\n",
            "Epoch 53/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5564 - accuracy: 0.7637\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.59932 to 0.59882, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.59932 to 0.59882, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00053: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00053: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5759 - accuracy: 0.7536 - val_loss: 0.5988 - val_accuracy: 0.7450 - lr: 1.0000e-04\n",
            "Epoch 54/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5736 - accuracy: 0.7534\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.59882 to 0.59779, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00054: val_loss improved from 0.59882 to 0.59779, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00054: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00054: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.5741 - accuracy: 0.7530 - val_loss: 0.5978 - val_accuracy: 0.7466 - lr: 1.0000e-04\n",
            "Epoch 55/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5538 - accuracy: 0.7793\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.59779 to 0.59605, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.59779 to 0.59605, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00055: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00055: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5722 - accuracy: 0.7571 - val_loss: 0.5961 - val_accuracy: 0.7463 - lr: 1.0000e-04\n",
            "Epoch 56/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5803 - accuracy: 0.7314\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.59605 to 0.59250, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.59605 to 0.59250, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00056: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00056: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5705 - accuracy: 0.7557 - val_loss: 0.5925 - val_accuracy: 0.7450 - lr: 1.0000e-04\n",
            "Epoch 57/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5989 - accuracy: 0.7510\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.59250 to 0.59115, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00057: val_loss improved from 0.59250 to 0.59115, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00057: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00057: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5680 - accuracy: 0.7564 - val_loss: 0.5911 - val_accuracy: 0.7447 - lr: 1.0000e-04\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5663 - accuracy: 0.7581\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.59115 to 0.58905, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00058: val_loss improved from 0.59115 to 0.58905, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00058: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00058: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5663 - accuracy: 0.7581 - val_loss: 0.5891 - val_accuracy: 0.7466 - lr: 1.0000e-04\n",
            "Epoch 59/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5353 - accuracy: 0.7539\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.58905 to 0.58740, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.58905 to 0.58740, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00059: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00059: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5647 - accuracy: 0.7566 - val_loss: 0.5874 - val_accuracy: 0.7445 - lr: 1.0000e-04\n",
            "Epoch 60/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5485 - accuracy: 0.7490\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.58740 to 0.58695, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00060: val_loss improved from 0.58740 to 0.58695, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00060: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00060: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00060: saving model to model_1/KERAS_check_model_epoch60.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.5634 - accuracy: 0.7577 - val_loss: 0.5869 - val_accuracy: 0.7474 - lr: 1.0000e-04\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5618 - accuracy: 0.7583\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.58695 to 0.58566, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00061: val_loss improved from 0.58695 to 0.58566, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00061: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00061: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.5618 - accuracy: 0.7583 - val_loss: 0.5857 - val_accuracy: 0.7479 - lr: 1.0000e-04\n",
            "Epoch 62/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5664 - accuracy: 0.7422\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.58566 to 0.58498, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00062: val_loss improved from 0.58566 to 0.58498, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00062: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00062: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5609 - accuracy: 0.7600 - val_loss: 0.5850 - val_accuracy: 0.7474 - lr: 1.0000e-04\n",
            "Epoch 63/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5731 - accuracy: 0.7451\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.58498 to 0.58389, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00063: val_loss improved from 0.58498 to 0.58389, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00063: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00063: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5587 - accuracy: 0.7615 - val_loss: 0.5839 - val_accuracy: 0.7487 - lr: 1.0000e-04\n",
            "Epoch 64/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5377 - accuracy: 0.7656\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.58389 to 0.58267, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.58389 to 0.58267, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00064: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00064: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5574 - accuracy: 0.7596 - val_loss: 0.5827 - val_accuracy: 0.7542 - lr: 1.0000e-04\n",
            "Epoch 65/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5932 - accuracy: 0.7373\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.58267 to 0.57969, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00065: val_loss improved from 0.58267 to 0.57969, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00065: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00065: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5572 - accuracy: 0.7579 - val_loss: 0.5797 - val_accuracy: 0.7466 - lr: 1.0000e-04\n",
            "Epoch 66/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5729 - accuracy: 0.7578\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.57969 to 0.57862, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.57969 to 0.57862, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00066: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00066: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5546 - accuracy: 0.7615 - val_loss: 0.5786 - val_accuracy: 0.7487 - lr: 1.0000e-04\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5535 - accuracy: 0.7592\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.57862\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.57862\n",
            "\n",
            "Epoch 00067: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00067: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5535 - accuracy: 0.7592 - val_loss: 0.5797 - val_accuracy: 0.7534 - lr: 1.0000e-04\n",
            "Epoch 68/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5644 - accuracy: 0.7441\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.57862 to 0.57688, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00068: val_loss improved from 0.57862 to 0.57688, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00068: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00068: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5531 - accuracy: 0.7603 - val_loss: 0.5769 - val_accuracy: 0.7513 - lr: 1.0000e-04\n",
            "Epoch 69/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.6001 - accuracy: 0.7510\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.57688 to 0.57642, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.57688 to 0.57642, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00069: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00069: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5510 - accuracy: 0.7616 - val_loss: 0.5764 - val_accuracy: 0.7503 - lr: 1.0000e-04\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5502 - accuracy: 0.7614\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.57642 to 0.57592, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.57642 to 0.57592, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00070: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00070: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00070: saving model to model_1/KERAS_check_model_epoch70.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.5502 - accuracy: 0.7614 - val_loss: 0.5759 - val_accuracy: 0.7584 - lr: 1.0000e-04\n",
            "Epoch 71/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5489 - accuracy: 0.7622\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.57592 to 0.57388, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.57592 to 0.57388, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00071: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00071: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5488 - accuracy: 0.7621 - val_loss: 0.5739 - val_accuracy: 0.7529 - lr: 1.0000e-04\n",
            "Epoch 72/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5022 - accuracy: 0.7871\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.57388 to 0.57299, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00072: val_loss improved from 0.57388 to 0.57299, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00072: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00072: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5484 - accuracy: 0.7620 - val_loss: 0.5730 - val_accuracy: 0.7524 - lr: 1.0000e-04\n",
            "Epoch 73/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5632 - accuracy: 0.7510\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.57299 to 0.57101, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.57299 to 0.57101, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00073: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00073: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5461 - accuracy: 0.7650 - val_loss: 0.5710 - val_accuracy: 0.7592 - lr: 1.0000e-04\n",
            "Epoch 74/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5336 - accuracy: 0.7822\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.57101\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.57101\n",
            "\n",
            "Epoch 00074: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00074: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5464 - accuracy: 0.7650 - val_loss: 0.5717 - val_accuracy: 0.7505 - lr: 1.0000e-04\n",
            "Epoch 75/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5365 - accuracy: 0.7695\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.57101\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.57101\n",
            "\n",
            "Epoch 00075: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00075: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5452 - accuracy: 0.7680 - val_loss: 0.5754 - val_accuracy: 0.7484 - lr: 1.0000e-04\n",
            "Epoch 76/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5669 - accuracy: 0.7461\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.57101 to 0.56952, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.57101 to 0.56952, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00076: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00076: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5443 - accuracy: 0.7659 - val_loss: 0.5695 - val_accuracy: 0.7626 - lr: 1.0000e-04\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5423 - accuracy: 0.7670\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.56952 to 0.56695, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.56952 to 0.56695, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00077: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00077: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.5423 - accuracy: 0.7670 - val_loss: 0.5669 - val_accuracy: 0.7571 - lr: 1.0000e-04\n",
            "Epoch 78/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5447 - accuracy: 0.7773\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.56695 to 0.56642, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00078: val_loss improved from 0.56695 to 0.56642, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00078: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00078: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5408 - accuracy: 0.7669 - val_loss: 0.5664 - val_accuracy: 0.7608 - lr: 1.0000e-04\n",
            "Epoch 79/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5366 - accuracy: 0.7744\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.56642\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.56642\n",
            "\n",
            "Epoch 00079: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00079: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.5397 - accuracy: 0.7681 - val_loss: 0.5689 - val_accuracy: 0.7508 - lr: 1.0000e-04\n",
            "Epoch 80/100\n",
            "10/12 [========================>.....] - ETA: 0s - loss: 0.5436 - accuracy: 0.7638\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.56642 to 0.56465, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00080: val_loss improved from 0.56642 to 0.56465, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00080: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00080: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00080: saving model to model_1/KERAS_check_model_epoch80.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.5413 - accuracy: 0.7657 - val_loss: 0.5647 - val_accuracy: 0.7545 - lr: 1.0000e-04\n",
            "Epoch 81/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5380 - accuracy: 0.7670\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.56465\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.56465\n",
            "\n",
            "Epoch 00081: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00081: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5380 - accuracy: 0.7670 - val_loss: 0.5650 - val_accuracy: 0.7579 - lr: 1.0000e-04\n",
            "Epoch 82/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5606 - accuracy: 0.7568\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.56465\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.56465\n",
            "\n",
            "Epoch 00082: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00082: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.5368 - accuracy: 0.7700 - val_loss: 0.5705 - val_accuracy: 0.7482 - lr: 1.0000e-04\n",
            "Epoch 83/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5365 - accuracy: 0.7549\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.56465 to 0.56362, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00083: val_loss improved from 0.56465 to 0.56362, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00083: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00083: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5404 - accuracy: 0.7638 - val_loss: 0.5636 - val_accuracy: 0.7642 - lr: 1.0000e-04\n",
            "Epoch 84/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5751 - accuracy: 0.7598\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.56362 to 0.56174, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00084: val_loss improved from 0.56362 to 0.56174, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00084: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00084: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5359 - accuracy: 0.7689 - val_loss: 0.5617 - val_accuracy: 0.7597 - lr: 1.0000e-04\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5350 - accuracy: 0.7686\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.56174\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.56174\n",
            "\n",
            "Epoch 00085: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00085: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.5350 - accuracy: 0.7686 - val_loss: 0.5620 - val_accuracy: 0.7695 - lr: 1.0000e-04\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.7721\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.56174 to 0.55975, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.56174 to 0.55975, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00086: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00086: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5352 - accuracy: 0.7721 - val_loss: 0.5597 - val_accuracy: 0.7587 - lr: 1.0000e-04\n",
            "Epoch 87/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5341 - accuracy: 0.7676\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.55975\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.55975\n",
            "\n",
            "Epoch 00087: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00087: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.5325 - accuracy: 0.7716 - val_loss: 0.5607 - val_accuracy: 0.7550 - lr: 1.0000e-04\n",
            "Epoch 88/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5442 - accuracy: 0.7607\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.55975\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.55975\n",
            "\n",
            "Epoch 00088: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00088: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.5328 - accuracy: 0.7707 - val_loss: 0.5600 - val_accuracy: 0.7579 - lr: 1.0000e-04\n",
            "Epoch 89/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5456 - accuracy: 0.7705\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.55975 to 0.55815, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00089: val_loss improved from 0.55975 to 0.55815, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00089: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00089: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5307 - accuracy: 0.7720 - val_loss: 0.5581 - val_accuracy: 0.7629 - lr: 1.0000e-04\n",
            "Epoch 90/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5180 - accuracy: 0.7842\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.55815\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.55815\n",
            "\n",
            "Epoch 00090: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00090: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00090: saving model to model_1/KERAS_check_model_epoch90.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5299 - accuracy: 0.7736 - val_loss: 0.5589 - val_accuracy: 0.7581 - lr: 1.0000e-04\n",
            "Epoch 91/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5279 - accuracy: 0.7730\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.55815 to 0.55669, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.55815 to 0.55669, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00091: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00091: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.5293 - accuracy: 0.7730 - val_loss: 0.5567 - val_accuracy: 0.7650 - lr: 1.0000e-04\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.7714\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.55669\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.55669\n",
            "\n",
            "Epoch 00092: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00092: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5291 - accuracy: 0.7714 - val_loss: 0.5571 - val_accuracy: 0.7687 - lr: 1.0000e-04\n",
            "Epoch 93/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5295 - accuracy: 0.7688\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.55669\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.55669\n",
            "\n",
            "Epoch 00093: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00093: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.5287 - accuracy: 0.7694 - val_loss: 0.5571 - val_accuracy: 0.7679 - lr: 1.0000e-04\n",
            "Epoch 94/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5476 - accuracy: 0.7725\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.55669\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.55669\n",
            "\n",
            "Epoch 00094: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00094: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.5278 - accuracy: 0.7722 - val_loss: 0.5578 - val_accuracy: 0.7702 - lr: 1.0000e-04\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5274 - accuracy: 0.7716\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.55669 to 0.55515, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.55669 to 0.55515, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00095: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00095: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5274 - accuracy: 0.7716 - val_loss: 0.5552 - val_accuracy: 0.7618 - lr: 1.0000e-04\n",
            "Epoch 96/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5234 - accuracy: 0.7762\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.55515\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.55515\n",
            "\n",
            "Epoch 00096: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00096: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.5249 - accuracy: 0.7756 - val_loss: 0.5565 - val_accuracy: 0.7571 - lr: 1.0000e-04\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - ETA: 0s - loss: 0.5267 - accuracy: 0.7732\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.55515 to 0.55419, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.55515 to 0.55419, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00097: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00097: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.5267 - accuracy: 0.7732 - val_loss: 0.5542 - val_accuracy: 0.7616 - lr: 1.0000e-04\n",
            "Epoch 98/100\n",
            " 9/12 [=====================>........] - ETA: 0s - loss: 0.5244 - accuracy: 0.7735\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.55419 to 0.55192, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.55419 to 0.55192, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00098: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00098: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5240 - accuracy: 0.7736 - val_loss: 0.5519 - val_accuracy: 0.7684 - lr: 1.0000e-04\n",
            "Epoch 99/100\n",
            "11/12 [==========================>...] - ETA: 0s - loss: 0.5220 - accuracy: 0.7767\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.55192 to 0.55168, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00099: val_loss improved from 0.55192 to 0.55168, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00099: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00099: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.5227 - accuracy: 0.7763 - val_loss: 0.5517 - val_accuracy: 0.7621 - lr: 1.0000e-04\n",
            "Epoch 100/100\n",
            " 1/12 [=>............................] - ETA: 0s - loss: 0.5447 - accuracy: 0.7637\n",
            "***callbacks***\n",
            "saving losses to model_1/losses.log\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.55168 to 0.55133, saving model to model_1/KERAS_check_best_model.h5\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.55168 to 0.55133, saving model to model_1/KERAS_check_best_model_weights.h5\n",
            "\n",
            "Epoch 00100: saving model to model_1/KERAS_check_model_last.h5\n",
            "\n",
            "Epoch 00100: saving model to model_1/KERAS_check_model_last_weights.h5\n",
            "\n",
            "Epoch 00100: saving model to model_1/KERAS_check_model_epoch100.h5\n",
            "\n",
            "***callbacks end***\n",
            "\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.5218 - accuracy: 0.7760 - val_loss: 0.5513 - val_accuracy: 0.7647 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY9N2usLTNop",
        "outputId": "ef33821c-3738-451a-b73a-db4ec8d6f95d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " fc1 (Dense)                 (None, 64)                704       \n",
            "                                                                 \n",
            " relu1 (Activation)          (None, 64)                0         \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 32)                2080      \n",
            "                                                                 \n",
            " relu2 (Activation)          (None, 32)                0         \n",
            "                                                                 \n",
            " fc3 (Dense)                 (None, 32)                1056      \n",
            "                                                                 \n",
            " relu3 (Activation)          (None, 32)                0         \n",
            "                                                                 \n",
            " output (Dense)              (None, 2)                 66        \n",
            "                                                                 \n",
            " softmax (Activation)        (None, 2)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,906\n",
            "Trainable params: 3,906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Predictions**"
      ],
      "metadata": {
        "id": "HIXInHinrBRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final part plans to know the accuracy of the model using the validation set."
      ],
      "metadata": {
        "id": "VFlbgEXrrCPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "TOzc5i2prKUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the model."
      ],
      "metadata": {
        "id": "9aH6EQbKtwqW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions are compared to the expected results in the validation set, then calculate classification accuracy."
      ],
      "metadata": {
        "id": "3PP40NK3rSkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "Euemn1zCrTmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xUhnu6EiT_D",
        "outputId": "02106d0f-114c-4bed-db23-4db4287905f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7744479495268138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classifying a background event as signal is worse than classifying a signal event as background. For comparison of different classifiers a ROC curve has to be used. "
      ],
      "metadata": {
        "id": "sXLPYkgXUM1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "import itertools\n",
        "from Plotting import makeRoc, plot_confusion_matrix, rocData"
      ],
      "metadata": {
        "id": "hQ4Cy5Cw8MFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the Receiver Operating Characteristic (ROC) curve"
      ],
      "metadata": {
        "id": "Vfz0JCkFtlOP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9,9))\n",
        "_ = makeRoc(y_test, predictions, le.classes_)"
      ],
      "metadata": {
        "id": "avnYyd0Q9EaN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "outputId": "516226ba-187c-42a7-8749-0c2cf7f84df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plot the confusion matrix**"
      ],
      "metadata": {
        "id": "G3w1_YPut1wn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion matrix provides an indication of the errors made.  It reports the number of true positives, false negatives, false positives, and true negatives. This allows more detailed analysis than simply observing the proportion of correct classifications (accuracy). Accuracy will yield misleading results if the data set is unbalanced; that is, when the numbers of observations in different classes vary greatly."
      ],
      "metadata": {
        "id": "SAvTes3VU6it"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"confusion matrix: {}\".format(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))))\n",
        "plt.figure(figsize=(9,9))\n",
        "_ = plot_confusion_matrix( confusion_matrix(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1)),le.classes_)"
      ],
      "metadata": {
        "id": "6KYSlRTD-Csi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "689c100a-162c-4d13-ddf7-5f0f9ef219c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix: [[2084  376]\n",
            " [ 482  862]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute  the Area Under the Curve (AUC) \n",
        "(information already present in the figure of the ROC curve)."
      ],
      "metadata": {
        "id": "YDs6gHrfuxVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a,u,c=rocData( y_test, predictions, le.classes_)"
      ],
      "metadata": {
        "id": "hnjC56SqHPhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(c)"
      ],
      "metadata": {
        "id": "7teapHWSIdOP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7be93acf-d9bb-43e9-9954-106ac08e592d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'g': 0.8241210559427024, 'h': 0.8241210559427022}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification report"
      ],
      "metadata": {
        "id": "bxJUxYP9VO0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classification report provides a breakdown of each class by precision, recall, f1-score(harmonic mean between precision and recall) and support(the number of samples of the true response that lie in that class) showing excellent results (granted the validation dataset was small)."
      ],
      "metadata": {
        "id": "X4cKVZSyVbdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"classification report: {}\".format(classification_report(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkQlXQfHUiBm",
        "outputId": "b55804a4-c173-4ba1-e6a2-87c3c0274b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.85      0.83      2460\n",
            "           1       0.70      0.64      0.67      1344\n",
            "\n",
            "    accuracy                           0.77      3804\n",
            "   macro avg       0.75      0.74      0.75      3804\n",
            "weighted avg       0.77      0.77      0.77      3804\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Create the model with pruning**"
      ],
      "metadata": {
        "id": "L1vvsglJjqNt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here it is applied pruning to the baseline model. \n",
        "The Neural Network model built untill now can be optimized via weight pruning, which consists of reducing the number of parameters and operations involved in the computation, by removing unnecessary connections (and then parameters), in between NN layers. This allows to minimize the resource utilization. \n",
        "The pruning is done during the training process to allow the NN to adapt to the changes."
      ],
      "metadata": {
        "id": "mASvlxsTk6Ut"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.python.keras.models import Model\n",
        "import tempfile\n",
        "! pip install -q tensorflow-model-optimization\n",
        "import tensorflow_model_optimization as tfmot\n",
        "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
        "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
        "from callbacks import all_callbacks\n",
        "from tensorflow.keras.callbacks import Callback, EarlyStopping,History\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "import tempfile\n",
        "logdir = tempfile.mkdtemp()\n",
        "pruning_params = {\" pruning_schedule \" : \n",
        "                  tfmot.sparsity.keras.ConstantSparsity(0.75,\n",
        "                  begin_step=2000,\n",
        "                  frequency=100)}\n",
        "model_for_pruning = prune_low_magnitude (model , **pruning_params )\n",
        "print(pruning_params)\n",
        "\n",
        "\n",
        "adam = Adam( lr =0.001)\n",
        "model_for_pruning.compile( loss=['categorical_crossentropy'], optimizer=adam)\n",
        "#model_for_pruning.compile( loss=tf.keras.losses.MeanSquaredError(), optimizer=adam)\n",
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue1Kgx-Ejqy3",
        "outputId": "a5b34fe4-3163-45fb-ed77-89cb46bf0294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |                              | 10 kB 22.2 MB/s eta 0:00:01\r\u001b[K     |                             | 20 kB 28.6 MB/s eta 0:00:01\r\u001b[K     |                           | 30 kB 15.8 MB/s eta 0:00:01\r\u001b[K     |                          | 40 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |                         | 51 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |                       | 61 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |                      | 71 kB 6.0 MB/s eta 0:00:01\r\u001b[K     |                    | 81 kB 6.7 MB/s eta 0:00:01\r\u001b[K     |                   | 92 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |                  | 102 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |                | 112 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |               | 122 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |             | 133 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |            | 143 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |           | 153 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |         | 163 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |        | 174 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |      | 184 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |     | 194 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |    | 204 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |  | 215 kB 5.3 MB/s eta 0:00:01\r\u001b[K     | | 225 kB 5.3 MB/s eta 0:00:01\r\u001b[K     || 234 kB 5.3 MB/s \n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:218: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  aggregation=tf.VariableAggregation.MEAN)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:225: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  aggregation=tf.VariableAggregation.MEAN)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_model_optimization/python/core/sparsity/keras/pruning_wrapper.py:238: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  trainable=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{' pruning_schedule ': <tensorflow_model_optimization.python.core.sparsity.keras.pruning_schedule.ConstantSparsity object at 0x7f138e547590>}\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " prune_low_magnitude_fc1 (Pr  (None, 64)               1346      \n",
            " uneLowMagnitude)                                                \n",
            "                                                                 \n",
            " prune_low_magnitude_relu1 (  (None, 64)               1         \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            " prune_low_magnitude_fc2 (Pr  (None, 32)               4130      \n",
            " uneLowMagnitude)                                                \n",
            "                                                                 \n",
            " prune_low_magnitude_relu2 (  (None, 32)               1         \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            " prune_low_magnitude_fc3 (Pr  (None, 32)               2082      \n",
            " uneLowMagnitude)                                                \n",
            "                                                                 \n",
            " prune_low_magnitude_relu3 (  (None, 32)               1         \n",
            " PruneLowMagnitude)                                              \n",
            "                                                                 \n",
            " prune_low_magnitude_output   (None, 2)                132       \n",
            " (PruneLowMagnitude)                                             \n",
            "                                                                 \n",
            " prune_low_magnitude_softmax  (None, 2)                1         \n",
            "  (PruneLowMagnitude)                                            \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,694\n",
            "Trainable params: 3,906\n",
            "Non-trainable params: 3,788\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model against baseline"
      ],
      "metadata": {
        "id": "VE1QibaTj-Y-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "logdir = tempfile.mkdtemp()\n",
        "\n",
        "callbacks = [\n",
        "  tfmot.sparsity.keras.UpdatePruningStep(),\n",
        "  tfmot.sparsity.keras.PruningSummaries(log_dir=logdir),\n",
        "]\n",
        "model_for_pruning.fit(X_train_val, y_train_val, \n",
        "                      batch_size=1028, \n",
        "                      epochs=100, \n",
        "                      validation_split=0.25,\n",
        "                      callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZsB_fCdj9QX",
        "outputId": "9458e581-fc8e-4229-c18e-c9b571e9fc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            " 1/12 [=>............................] - ETA: 1:18 - loss: 0.5146WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0069s vs `on_train_batch_end` time: 0.0181s). Check your callbacks.\n",
            "12/12 [==============================] - 8s 50ms/step - loss: 1.3245 - val_loss: 0.9580\n",
            "Epoch 2/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.7617 - val_loss: 0.6693\n",
            "Epoch 3/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.6054 - val_loss: 0.5799\n",
            "Epoch 4/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.5498 - val_loss: 0.5533\n",
            "Epoch 5/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.5261 - val_loss: 0.5459\n",
            "Epoch 6/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.5211 - val_loss: 0.5385\n",
            "Epoch 7/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.5146 - val_loss: 0.5531\n",
            "Epoch 8/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 0.5185 - val_loss: 0.5435\n",
            "Epoch 9/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 2.5258 - val_loss: 2.7200\n",
            "Epoch 10/100\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 1.4208 - val_loss: 1.0726\n",
            "Epoch 11/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.8630 - val_loss: 0.7495\n",
            "Epoch 12/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.7047 - val_loss: 0.6017\n",
            "Epoch 13/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.6056 - val_loss: 0.5894\n",
            "Epoch 14/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5724 - val_loss: 0.5718\n",
            "Epoch 15/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.5525 - val_loss: 0.5634\n",
            "Epoch 16/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.5415 - val_loss: 0.5542\n",
            "Epoch 17/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.5348 - val_loss: 0.5457\n",
            "Epoch 18/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5253 - val_loss: 0.5394\n",
            "Epoch 19/100\n",
            "12/12 [==============================] - 0s 22ms/step - loss: 0.5199 - val_loss: 0.5345\n",
            "Epoch 20/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.5149 - val_loss: 0.5332\n",
            "Epoch 21/100\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 0.5114 - val_loss: 0.5274\n",
            "Epoch 22/100\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.5093 - val_loss: 0.5215\n",
            "Epoch 23/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.5078 - val_loss: 0.5209\n",
            "Epoch 24/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.5029 - val_loss: 0.5199\n",
            "Epoch 25/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.4993 - val_loss: 0.5156\n",
            "Epoch 26/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.4959 - val_loss: 0.5131\n",
            "Epoch 27/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.4935 - val_loss: 0.5132\n",
            "Epoch 28/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.4921 - val_loss: 0.5147\n",
            "Epoch 29/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.4910 - val_loss: 0.5093\n",
            "Epoch 30/100\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 0.4944 - val_loss: 0.5045\n",
            "Epoch 31/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.4863 - val_loss: 0.5068\n",
            "Epoch 32/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4826 - val_loss: 0.5004\n",
            "Epoch 33/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.4796 - val_loss: 0.4980\n",
            "Epoch 34/100\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 0.4815 - val_loss: 0.4936\n",
            "Epoch 35/100\n",
            "12/12 [==============================] - 0s 24ms/step - loss: 0.4764 - val_loss: 0.4934\n",
            "Epoch 36/100\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 0.4754 - val_loss: 0.4957\n",
            "Epoch 37/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.4749 - val_loss: 0.4953\n",
            "Epoch 38/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.4707 - val_loss: 0.4894\n",
            "Epoch 39/100\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 0.4691 - val_loss: 0.4939\n",
            "Epoch 40/100\n",
            "12/12 [==============================] - 0s 16ms/step - loss: 0.4684 - val_loss: 0.4884\n",
            "Epoch 41/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.4668 - val_loss: 0.4863\n",
            "Epoch 42/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.4648 - val_loss: 0.4838\n",
            "Epoch 43/100\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 0.4650 - val_loss: 0.4822\n",
            "Epoch 44/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.4600 - val_loss: 0.4837\n",
            "Epoch 45/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4675 - val_loss: 0.4773\n",
            "Epoch 46/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4583 - val_loss: 0.4767\n",
            "Epoch 47/100\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 0.4574 - val_loss: 0.4744\n",
            "Epoch 48/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4553 - val_loss: 0.4806\n",
            "Epoch 49/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4565 - val_loss: 0.4847\n",
            "Epoch 50/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4585 - val_loss: 0.4766\n",
            "Epoch 51/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4528 - val_loss: 0.4712\n",
            "Epoch 52/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4547 - val_loss: 0.4685\n",
            "Epoch 53/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4509 - val_loss: 0.4702\n",
            "Epoch 54/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4474 - val_loss: 0.4675\n",
            "Epoch 55/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4505 - val_loss: 0.4787\n",
            "Epoch 56/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4466 - val_loss: 0.4669\n",
            "Epoch 57/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4448 - val_loss: 0.4654\n",
            "Epoch 58/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4442 - val_loss: 0.4673\n",
            "Epoch 59/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4428 - val_loss: 0.4621\n",
            "Epoch 60/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4423 - val_loss: 0.4659\n",
            "Epoch 61/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4402 - val_loss: 0.4658\n",
            "Epoch 62/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4420 - val_loss: 0.4613\n",
            "Epoch 63/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4387 - val_loss: 0.4631\n",
            "Epoch 64/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4373 - val_loss: 0.4588\n",
            "Epoch 65/100\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4350 - val_loss: 0.4834\n",
            "Epoch 66/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4569 - val_loss: 0.4690\n",
            "Epoch 67/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4414 - val_loss: 0.4616\n",
            "Epoch 68/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4335 - val_loss: 0.4580\n",
            "Epoch 69/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4343 - val_loss: 0.4565\n",
            "Epoch 70/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4321 - val_loss: 0.4567\n",
            "Epoch 71/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4304 - val_loss: 0.4558\n",
            "Epoch 72/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4313 - val_loss: 0.4544\n",
            "Epoch 73/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4284 - val_loss: 0.4538\n",
            "Epoch 74/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4332 - val_loss: 0.4526\n",
            "Epoch 75/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4283 - val_loss: 0.4554\n",
            "Epoch 76/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4270 - val_loss: 0.4500\n",
            "Epoch 77/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4263 - val_loss: 0.4570\n",
            "Epoch 78/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4269 - val_loss: 0.4511\n",
            "Epoch 79/100\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 0.4263 - val_loss: 0.4545\n",
            "Epoch 80/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4246 - val_loss: 0.4522\n",
            "Epoch 81/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4281 - val_loss: 0.4517\n",
            "Epoch 82/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4249 - val_loss: 0.4660\n",
            "Epoch 83/100\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 0.4304 - val_loss: 0.4644\n",
            "Epoch 84/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4288 - val_loss: 0.4484\n",
            "Epoch 85/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4237 - val_loss: 0.4486\n",
            "Epoch 86/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4233 - val_loss: 0.4486\n",
            "Epoch 87/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4213 - val_loss: 0.4521\n",
            "Epoch 88/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4211 - val_loss: 0.4491\n",
            "Epoch 89/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4215 - val_loss: 0.4548\n",
            "Epoch 90/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4239 - val_loss: 0.4572\n",
            "Epoch 91/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4210 - val_loss: 0.4488\n",
            "Epoch 92/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4223 - val_loss: 0.4487\n",
            "Epoch 93/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4198 - val_loss: 0.4511\n",
            "Epoch 94/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4182 - val_loss: 0.4498\n",
            "Epoch 95/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4186 - val_loss: 0.4456\n",
            "Epoch 96/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4176 - val_loss: 0.4570\n",
            "Epoch 97/100\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4197 - val_loss: 0.4483\n",
            "Epoch 98/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4174 - val_loss: 0.4439\n",
            "Epoch 99/100\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4165 - val_loss: 0.4579\n",
            "Epoch 100/100\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4183 - val_loss: 0.4464\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f138c2c9a10>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preditions"
      ],
      "metadata": {
        "id": "w7dIWNuIkPqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pruned_predictions = model_for_pruning.predict(np.ascontiguousarray(X_test))"
      ],
      "metadata": {
        "id": "GDA55vyRkOwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation of the model against the baseline."
      ],
      "metadata": {
        "id": "wG59AluMtdy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy after pruning improves:"
      ],
      "metadata": {
        "id": "C4RGTs1bkaSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy with pruned model: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_pruned_predictions, axis=1))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iICbgD0TkZR1",
        "outputId": "e85ccda8-1b60-444b-8681-e0a4c7efa28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with pruned model: 0.8275499474237644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ROC curve"
      ],
      "metadata": {
        "id": "d31A6Adki29M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(9,9))\n",
        "_ = makeRoc(y_test, y_pruned_predictions, le.classes_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        },
        "id": "bvV5FNbWi2hV",
        "outputId": "1ab2f596-5d6b-408d-b627-51082ca3abe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x648 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix"
      ],
      "metadata": {
        "id": "t_DKwMuNsdQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"confusion matrix: {}\".format(confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pruned_predictions, axis=1))))\n",
        "plt.figure(figsize=(9,9))\n",
        "_ = plot_confusion_matrix( confusion_matrix(np.argmax(y_test, axis=1), np.argmax(y_pruned_predictions, axis=1)),le.classes_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "YH6sSHwajNqS",
        "outputId": "9a0118d6-7932-428a-df53-7edabf98c01f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix: [[2274  186]\n",
            " [ 470  874]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x648 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification report"
      ],
      "metadata": {
        "id": "wcyQkinPsZey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"classification report: {}\".format(classification_report(np.argmax(y_test, axis=1), np.argmax(y_pruned_predictions, axis=1))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ00hWfgsQV5",
        "outputId": "cbbfcb52-25e7-4e67-e593-11b88b4ac8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classification report:               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87      2460\n",
            "           1       0.82      0.65      0.73      1344\n",
            "\n",
            "    accuracy                           0.83      3804\n",
            "   macro avg       0.83      0.79      0.80      3804\n",
            "weighted avg       0.83      0.83      0.82      3804\n",
            "\n"
          ]
        }
      ]
    }
  ]
}